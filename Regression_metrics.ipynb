{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMijZCD7GI3JoFeZeJak/uO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ehtisham1053/Regression-ML-Algorithms/blob/main/Regression_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EVWyg9qy31Rb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('audi.csv')"
      ],
      "metadata": {
        "id": "dyBKTVrw35sz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.drop('price', axis=1)\n",
        "y = df['price']"
      ],
      "metadata": {
        "id": "xQQkYtYh4AgY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "GmkMVaqO4BwB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = ColumnTransformer(transformers=[\n",
        "(\"onehot\" , OneHotEncoder(handle_unknown='ignore' , sparse_output=False , drop='first') , ['model' , 'transmission' , 'fuelType']),\n",
        "                                    (\"scaling\" , StandardScaler() , ['mileage' , 'tax' , 'mpg' , 'engineSize'])\n",
        "],\n",
        "                      remainder='passthrough')"
      ],
      "metadata": {
        "id": "0-NOcGgL4G_w"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = c.fit_transform(x_train)\n",
        "x_test = c.transform(x_test)\n"
      ],
      "metadata": {
        "id": "xChgQhQb4WhQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# implementing the regression metrics using python class"
      ],
      "metadata": {
        "id": "juLGI0Ak5IUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class LinearRegressionManual:\n",
        "    def __init__(self, x_train, y_train, x_test, y_test):\n",
        "        self.x_train = np.array(x_train)\n",
        "        self.y_train = np.array(y_train)\n",
        "        self.x_test = np.array(x_test)\n",
        "        self.y_test = np.array(y_test)\n",
        "        self.m = None\n",
        "        self.c = None\n",
        "        self.y_pred = None\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the model using OLS method\"\"\"\n",
        "        X = np.c_[np.ones(self.x_train.shape[0]), self.x_train]\n",
        "        theta = np.linalg.inv(X.T @ X) @ X.T @ self.y_train\n",
        "        self.c = theta[0]\n",
        "        self.m = theta[1:]\n",
        "\n",
        "    def predict(self):\n",
        "        \"\"\"Make predictions using the trained model\"\"\"\n",
        "        X_test = np.c_[np.ones(self.x_test.shape[0]), self.x_test]\n",
        "        self.y_pred = X_test @ np.r_[self.c, self.m]\n",
        "        return self.y_pred\n",
        "\n",
        "    def mse(self):\n",
        "        \"\"\"Mean Squared Error\"\"\"\n",
        "        return np.mean((self.y_test - self.y_pred) ** 2)\n",
        "\n",
        "    def mae(self):\n",
        "        \"\"\"Mean Absolute Error\"\"\"\n",
        "        return np.mean(np.abs(self.y_test - self.y_pred))\n",
        "\n",
        "    def rmse(self):\n",
        "        \"\"\"Root Mean Squared Error\"\"\"\n",
        "        return np.sqrt(self.mse())\n",
        "\n",
        "    def r2_score(self):\n",
        "        \"\"\"R¬≤ Score\"\"\"\n",
        "        ss_total = np.sum((self.y_test - np.mean(self.y_test)) ** 2)\n",
        "        ss_residual = np.sum((self.y_test - self.y_pred) ** 2)\n",
        "        return 1 - (ss_residual / ss_total)\n",
        "\n",
        "    def adjusted_r2(self):\n",
        "        \"\"\"Adjusted R¬≤ Score\"\"\"\n",
        "        n = len(self.y_test)\n",
        "        p = self.x_test.shape[1]\n",
        "        r2 = self.r2_score()\n",
        "        return 1 - ((1 - r2) * (n - 1) / (n - p - 1))\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Print evaluation metrics\"\"\"\n",
        "        print(f\"MSE: {self.mse():.4f}\")\n",
        "        print(f\"MAE: {self.mae():.4f}\")\n",
        "        print(f\"RMSE: {self.rmse():.4f}\")\n",
        "        print(f\"R¬≤ Score: {self.r2_score():.4f}\")\n",
        "        print(f\"Adjusted R¬≤ Score: {self.adjusted_r2():.4f}\")\n",
        "\n",
        "    def plot_regression(self):\n",
        "        \"\"\"Plot the regression line (only for single feature regression)\"\"\"\n",
        "        if self.x_test.shape[1] == 1:\n",
        "            plt.scatter(self.x_test, self.y_test, color=\"blue\", label=\"Actual Data\")\n",
        "            plt.plot(self.x_test, self.y_pred, color=\"red\", linewidth=2, label=\"Regression Line\")\n",
        "            plt.xlabel(\"X Test\")\n",
        "            plt.ylabel(\"Y Test\")\n",
        "            plt.legend()\n",
        "            plt.title(\"Linear Regression Model\")\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Plotting is only supported for single feature regression.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o_zN6hP15DB6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegressionManual(x_train, y_train, x_test, y_test)\n",
        "model.train()\n",
        "model.predict()\n",
        "model.evaluate()\n",
        "model.plot_regression()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvCvgcMs79Hz",
        "outputId": "f7e08672-196c-48b9-b23b-a086b2f6f4ce"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 15807273.4085\n",
            "MAE: 2605.5566\n",
            "RMSE: 3975.8362\n",
            "R¬≤ Score: 0.8813\n",
            "Adjusted R¬≤ Score: 0.8793\n",
            "Plotting is only supported for single feature regression.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üìå Linear Regression (Manual Implementation) ‚Äì Explanation\n",
        "Linear Regression is a fundamental algorithm used in machine learning to predict a continuous value based on input features. It establishes a linear relationship between the dependent variable (target) and independent variable(s) (features). The objective is to find the best-fit line that minimizes the difference between the actual and predicted values.\n",
        "\n",
        "*  1Ô∏è‚É£ Training the Model (Finding Coefficients)\n",
        "The model learns by calculating the slope (weights) and intercept, which define the best-fit line. These values are obtained by minimizing the total error, ensuring the predictions are as close as possible to the actual values.\n",
        "\n",
        "* 2Ô∏è‚É£ Making Predictions\n",
        "Once the model is trained, predictions are made by plugging the input values into the learned equation. This helps estimate outputs for new, unseen data.\n",
        "\n",
        "## üìå Regression Metrics (Manual Implementation)\n",
        "To evaluate how well the model performs, we use various metrics to measure the error and accuracy.\n",
        "\n",
        "* 1Ô∏è‚É£ Mean Squared Error (MSE)\n",
        "This metric calculates the average squared differences between actual and predicted values. Squaring the errors ensures that large deviations contribute more to the total error, making it useful for detecting large mistakes. Lower values indicate better model performance.\n",
        "\n",
        "* 2Ô∏è‚É£ Mean Absolute Error (MAE)\n",
        "Unlike MSE, this metric calculates the average absolute differences without squaring them. It provides a straightforward measure of how far predictions are from the actual values, making it more interpretable.\n",
        "\n",
        "* 3Ô∏è‚É£ Root Mean Squared Error (RMSE)\n",
        "This metric is similar to MSE but takes the square root of the result to bring the error back to the same scale as the target variable. It helps understand the impact of errors in real-world scenarios.\n",
        "\n",
        "* 4Ô∏è‚É£ R¬≤ Score (Coefficient of Determination)\n",
        "This metric evaluates how well the independent variables explain the variance in the dependent variable. A value close to 1 means the model fits well, while a value closer to 0 suggests the model does not explain the data effectively.\n",
        "\n",
        "* 5Ô∏è‚É£ Adjusted R¬≤ Score\n",
        "This metric improves upon the R¬≤ score by adjusting for the number of input features. It ensures that adding unnecessary features does not falsely inflate the model‚Äôs accuracy. This is particularly useful in models with multiple input variables.\n",
        "\n",
        "###üìå Model Evaluation\n",
        "After training the model and making predictions, we use these metrics to assess performance. If the error values are high, it may indicate issues such as underfitting or overfitting, requiring further adjustments to improve accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "Iym1M06k9sSL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GaALlgPI5EfP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation using sklearn library"
      ],
      "metadata": {
        "id": "GXEColpt-XYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "class SklearnRegressionMetrics:\n",
        "    def __init__(self, x_train, y_train, x_test, y_test):\n",
        "        \"\"\"\n",
        "        Initialize with preprocessed training and testing data.\n",
        "        Train the Linear Regression model using Sklearn.\n",
        "        \"\"\"\n",
        "        self.x_train = np.array(x_train)\n",
        "        self.y_train = np.array(y_train)\n",
        "        self.x_test = np.array(x_test)\n",
        "        self.y_test = np.array(y_test)\n",
        "\n",
        "        self.model = LinearRegression()\n",
        "        self.model.fit(self.x_train, self.y_train)\n",
        "        self.y_pred = self.model.predict(self.x_test)\n",
        "\n",
        "        self.n_test = len(self.x_test)\n",
        "        self.k = self.x_train.shape[1] if self.x_train.ndim > 1 else 1\n",
        "\n",
        "    def mse(self):\n",
        "        \"\"\" Mean Squared Error (MSE) \"\"\"\n",
        "        return mean_squared_error(self.y_test, self.y_pred)\n",
        "\n",
        "    def mae(self):\n",
        "        \"\"\" Mean Absolute Error (MAE) \"\"\"\n",
        "        return mean_absolute_error(self.y_test, self.y_pred)\n",
        "\n",
        "    def rmse(self):\n",
        "        \"\"\" Root Mean Squared Error (RMSE) \"\"\"\n",
        "        return np.sqrt(self.mse())\n",
        "\n",
        "    def r2_score(self):\n",
        "        \"\"\" R¬≤ Score \"\"\"\n",
        "        return r2_score(self.y_test, self.y_pred)\n",
        "\n",
        "    def adjusted_r2(self):\n",
        "        \"\"\" Adjusted R¬≤ Score \"\"\"\n",
        "        r2 = self.r2_score()\n",
        "        return 1 - ((1 - r2) * (self.n_test - 1) / (self.n_test - self.k - 1))\n",
        "\n",
        "    def print_metrics(self):\n",
        "        \"\"\" Print all regression metrics \"\"\"\n",
        "        print(f\"Slope (m): {self.model.coef_}\")\n",
        "        print(f\"Intercept (c): {self.model.intercept_}\")\n",
        "        print(f\"MSE: {self.mse():.4f}\")\n",
        "        print(f\"MAE: {self.mae():.4f}\")\n",
        "        print(f\"RMSE: {self.rmse():.4f}\")\n",
        "        print(f\"R¬≤ Score: {self.r2_score():.4f}\")\n",
        "        print(f\"Adjusted R¬≤ Score: {self.adjusted_r2():.4f}\")\n"
      ],
      "metadata": {
        "id": "kZVsUIaS5iKA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression = SklearnRegressionMetrics(x_train, y_train, x_test, y_test)\n",
        "regression.print_metrics()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "065BQjj06zVz",
        "outputId": "6dabcc30-8cf5-4a9a-b3bb-86e6a0af6a36"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (m): [ 1.92178629e+04  1.29739314e+03  1.54180779e+03  2.96542649e+03\n",
            "  3.38921603e+03  4.78655410e+03  7.78215999e+03  1.25767636e+03\n",
            "  2.84651695e+03  6.64017522e+03  1.52803428e+04  2.40532932e+04\n",
            "  6.29106831e+04  9.61851279e+03  2.07114640e+04  2.07030990e+04\n",
            "  2.61524962e+04  1.85973608e+04  3.98577106e+03  8.95570881e+03\n",
            "  2.56006846e+03  9.48877896e+03  1.02110331e+04  1.80500328e+04\n",
            "  3.52883742e+03 -1.50516734e+03  5.04928184e+01  3.38360445e+04\n",
            " -9.22835957e+02 -1.81255447e+03 -1.91903304e+03 -3.81772774e+03\n",
            "  2.72687711e+03  1.83974092e+03]\n",
            "Intercept (c): -3690450.7937985533\n",
            "MSE: 15807273.4084\n",
            "MAE: 2605.5566\n",
            "RMSE: 3975.8362\n",
            "R¬≤ Score: 0.8813\n",
            "Adjusted R¬≤ Score: 0.8793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OPlw1gxT-qbX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}